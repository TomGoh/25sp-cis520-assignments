{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgP7Hn45Ux6m"
      },
      "source": [
        "#CIS 5200: Machine Learning Homework 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5QrlvUpVoYw",
        "outputId": "3ad354d8-dc39-46ca-b164-d323a7a3da8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO, OK] Google Colab.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# For autograder only, do not modify this cell.\n",
        "# True for Google Colab, False for autograder\n",
        "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
        "if NOTEBOOK:\n",
        "    print(\"[INFO, OK] Google Colab.\")\n",
        "else:\n",
        "    print(\"[INFO, OK] Autograder.\")\n",
        "    sys.exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpPcJHGrVnSg",
        "outputId": "9abd638c-ac00-41ba-d709-b2bfc9d4c65c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: penngrader-client in /opt/homebrew/Caskroom/miniconda/base/envs/mlenv/lib/python3.12/site-packages (0.5.2)\n",
            "Requirement already satisfied: dill in /opt/homebrew/Caskroom/miniconda/base/envs/mlenv/lib/python3.12/site-packages (from penngrader-client) (0.3.9)\n",
            "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniconda/base/envs/mlenv/lib/python3.12/site-packages (from penngrader-client) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "!pip install penngrader-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuzOdblNVldu",
        "outputId": "f5a44cb1-5032-4bed-b252-8bc17e91445b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.yaml\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3819LJlAWJZM",
        "outputId": "7c5327c4-48b7-43ed-937d-93a70de60823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PennGrader initialized with Student ID: 17994725\n",
            "\n",
            "Make sure this correct or we will not be able to store your grade\n"
          ]
        }
      ],
      "source": [
        "from penngrader.grader import PennGrader\n",
        "\n",
        "# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "# TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 17994725 # YOUR PENN-ID GOES HERE AS AN INTEGER #\n",
        "SECRET = STUDENT_ID\n",
        "\n",
        "grader = PennGrader('config.yaml', 'cis5200_sp25_HW2', STUDENT_ID, SECRET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zreeB_llVvYg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn import datasets\n",
        "import cvxpy as cp\n",
        "\n",
        "from PIL import Image\n",
        "from dill.source import getsource\n",
        "\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_iwVpB8VyPT"
      },
      "source": [
        "# Dataset: Wine Quality Prediction\n",
        "\n",
        "Some research on blind wine tasting has suggested that [people cannot taste the difference between ordinary and pricy wine brands](https://phys.org/news/2011-04-expensive-inexpensive-wines.html). Indeed, even experienced tasters may be as consistent as [random numbers](https://www.seattleweekly.com/food/wine-snob-scandal/). Is professional wine tasting in shambles? Maybe ML can take over.\n",
        "\n",
        "In this problem set, we will train some simple linear models to predict wine quality. We'll be using the data from [this repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality) for both the classification and regression tasks. The following cells will download and set up the data for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zxytehGmV6o2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C5Hq9u6RV7_Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "red_df = pd.read_csv('winequality-red.csv', delimiter=';')\n",
        "\n",
        "X = torch.from_numpy(red_df.drop(columns=['quality']).to_numpy())\n",
        "y = torch.from_numpy(red_df['quality'].to_numpy())\n",
        "\n",
        "# Split data into train/test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data to have zero mean and standard deviation,\n",
        "# and add bias term\n",
        "mu, sigma = X_train.mean(0), X_train.std(0)\n",
        "X_train, X_test = [ torch.cat([((x-mu)/sigma).float(), torch.ones(x.size(0),1)], dim=1)\n",
        "                    for x in [X_train, X_test]]\n",
        "\n",
        "# Transform labels to {-1,1} for logistic regression\n",
        "y_binary_train, y_binary_test = [ (torch.sign(y - 5.5)).long()\n",
        "                                  for y in [y_train, y_test]]\n",
        "y_regression_train, y_regression_test = [ y.float() for y in [y_train, y_test]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7xzkqbXU59R"
      },
      "source": [
        "#1. Logistic Regression\n",
        "In this first problem, you will implement a logistic regression classifier to classify good wine (y=1) from bad wine (y=0). Your professor has arbitrarily decided that good wine has a score of at least 5.5. The classifier is split into the following components:\n",
        "\n",
        "Loss (3pts) & gradient (3pts) - given a batch of examples  \n",
        "X\n",
        "  and labels  \n",
        "y\n",
        "  and weights for the logistic regression classifier, compute the batched logistic loss and gradient of the loss with with respect to the model parameters  \n",
        "w\n",
        " . Note that this is slightly different from the gradient in Homework 0, which was with respect to the sample  \n",
        "X\n",
        " .\n",
        "Fit (2pt) - Given a loss function and data, find the weights of an optimal logistic regression model that minimizes the logistic loss\n",
        "Predict (3pts) - Given the weights of a logistic regression model and new data, predict the most likely class\n",
        "We provide an generic gradient-based optimizer for you which minimizes the logistic loss function, you can call it with LogisticOptimizer().optimize(X,y). It does not need any parameter adjustment.\n",
        "\n",
        "Hint: The optimizer will minimize the logistic loss. So this value of this loss should be decreasing over iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "m8MFAutvUuvs"
      },
      "outputs": [],
      "source": [
        "def logistic_loss(X, y, w):\n",
        "    # Given a batch of samples and labels, and the weights of a logistic\n",
        "    # classifier, compute the batched logistic loss.\n",
        "    #\n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n",
        "    #     of dimension d\n",
        "    #\n",
        "    # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n",
        "    #\n",
        "    # w := Tensor(float) of size(d,) --- This is the weights of a logistic\n",
        "    #     classifer.\n",
        "    #\n",
        "    # Return := Tensor of size (m,) --- This is the logistic loss for each\n",
        "    #     example.\n",
        "\n",
        "    # Fill in the rest\n",
        "    return torch.log(1+torch.exp(-y*torch.matmul(X,w)))\n",
        "\n",
        "def logistic_gradient(X, y, w):\n",
        "    # Given a batch of samples and labels, compute the batched gradient of\n",
        "    # the logistic loss.\n",
        "    #\n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n",
        "    #     of dimension d\n",
        "    #\n",
        "    # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n",
        "    #\n",
        "    # w := Tensor(float) of size(d,) --- This is the weights of a logistic\n",
        "    #     classifer.\n",
        "    #\n",
        "    # Return := Tensor of size (m,) --- This is the logistic loss for each\n",
        "    #     example.\n",
        "    #\n",
        "    # Hint: A very similar gradient was calculated in Homework 0.\n",
        "    # However, that was the sample gradient (with respect to X), whereas\n",
        "    # what we need here is the parameter gradient (with respect to w).\n",
        "\n",
        "    # Fill in the rest\n",
        "    output = torch.matmul(X, w)\n",
        "    return X*(-y * torch.exp(-y * output) / (1 + torch.exp(-y * output))).reshape(-1,1)\n",
        "\n",
        "\n",
        "def optimize(X, y, niters=100):\n",
        "    # Given a dataset of examples and labels, minimizes the logistic loss\n",
        "    # using standard gradient descent.\n",
        "    #\n",
        "    # This optimizer is written for you, and you only need to implement the\n",
        "    # logistic loss and gradient functions above.\n",
        "    #\n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n",
        "    #     of dimension d\n",
        "    #\n",
        "    # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n",
        "    #\n",
        "    # Return := Tensor of size(d,) --- This is the fitted weights of a\n",
        "    #     logistic regression model\n",
        "\n",
        "    m,d = X.size()\n",
        "    w = torch.zeros(d)\n",
        "    print('Optimizing logistic function...')\n",
        "    for i in range(niters):\n",
        "        loss = logistic_loss(X,y,w).mean()\n",
        "        grad = logistic_gradient(X,y,w).mean(0)\n",
        "        w -= grad\n",
        "        if i % 50 == 0:\n",
        "            print(i, loss.item())\n",
        "    print('Optimizing done.')\n",
        "    return w\n",
        "\n",
        "def logistic_fit(X, y):\n",
        "    # Given a dataset of examples and labels, fit the weights of the logistic\n",
        "    # regression classifier using the provided loss function and optimizer\n",
        "    #\n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n",
        "    #     of dimension d\n",
        "    #\n",
        "    # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n",
        "    #\n",
        "    # Return := Tensor of size (d,) --- This is the fitted weights of the\n",
        "    #     logistic regression model\n",
        "    import torch\n",
        "\n",
        "    # Fill in the rest. Hint -- call optimize :-).\n",
        "    return optimize(X,y)\n",
        "\n",
        "def logistic_predict(X, w):\n",
        "    # Given a dataset of examples and fitted weights for a logistic regression\n",
        "    # classifier, predict the class\n",
        "    #\n",
        "    # X := Tensor(float) of size(m,d) --- This is a batch of m examples of\n",
        "    #    dimension d\n",
        "    #\n",
        "    # w := Tensor(float) of size (d,) --- This is the fitted weights of the\n",
        "    #    logistic regression model\n",
        "    #\n",
        "    # Return := Tensor of size (m,) --- This is the predicted classes {-1,1}\n",
        "    #    for each example\n",
        "    #\n",
        "    # Hint: Remember that logistic regression expects a label in {-1,1}, and\n",
        "    # not {0,1}\n",
        "\n",
        "    # Fill in the rest\n",
        "    return torch.sign(torch.matmul(X,w))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ekOjVtYSV1Mc",
        "outputId": "e3825994-e5f2-44c9-dc0d-09df9e8e91eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing logistic function...\n",
            "0 0.6931473016738892\n",
            "50 0.5182141661643982\n",
            "Optimizing done.\n",
            "Train accuracy [zero]: 0.00\n",
            "Test accuracy [zero]: 0.00\n",
            "Train accuracy [random]: 0.54\n",
            "Test accuracy [random]: 0.53\n",
            "Train accuracy [fitted]: 0.75\n",
            "Test accuracy [fitted]: 0.75\n"
          ]
        }
      ],
      "source": [
        "# Test your code on the wine dataset!\n",
        "# How does your solution compare to a random linear classifier?\n",
        "# Your solution should get around 75% accuracy on the test set.\n",
        "torch.manual_seed(42)\n",
        "\n",
        "d = X_train.size(1)\n",
        "logistic_weights = {\n",
        "    'zero': torch.zeros(d),\n",
        "    'random': torch.randn(d),\n",
        "    'fitted': logistic_fit(X_train, y_binary_train)\n",
        "}\n",
        "\n",
        "for k,w in logistic_weights.items():\n",
        "    yp_binary_train = logistic_predict(X_train, w)\n",
        "    acc_train = (yp_binary_train == y_binary_train).float().mean()\n",
        "\n",
        "    print(f'Train accuracy [{k}]: {acc_train.item():.2f}')\n",
        "\n",
        "    yp_binary_test = logistic_predict(X_test, w)\n",
        "    acc_test = (yp_binary_test == y_binary_test).float().mean()\n",
        "\n",
        "    print(f'Test accuracy [{k}]: {acc_test.item():.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltnDMWEAV2dK",
        "outputId": "037a642e-eddc-4ef8-c8e2-51890861178b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "grader.grade(test_case_id = 'logistic_loss', answer = getsource(logistic_loss))\n",
        "grader.grade(test_case_id = 'logistic_gradient', answer = getsource(logistic_gradient))\n",
        "grader.grade(test_case_id = 'logistic_fit', answer = getsource(logistic_fit))\n",
        "grader.grade(test_case_id = 'logistic_predict', answer = getsource(logistic_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6tdHuKJVA1G"
      },
      "source": [
        "# 2. Linear Regression with Ridge Regression\n",
        "\n",
        "In this second problem, you'll implement a linear regression model. Similarly to the first problem, implement the following functions:\n",
        "\n",
        "1. Loss (3pts) - Given a batch of examples $X$ and labels $y$, compute the batched mean squared error loss for a linear model with weights $w$.\n",
        "2. Fit (4pts) - Given a batch of examples $X$ and labels $y$, find the weights of the optimal linear regression model\n",
        "3. Predict (3pts) - Given the weights $w$ of a linear regression model and new data $X$, predict the most likely label\n",
        "\n",
        "This time, you are not given an optimizer for the fitting function since this problem has an analytic solution. Make sure to test your solution with non-zero ridge regression parameters.\n",
        "\n",
        "Hint: You may want to review ridge regression on Slide 22 of Lecture 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4bM7an3vVBbo"
      },
      "outputs": [],
      "source": [
        "def regression_loss(X, y, w):\n",
        "    # Given a batch of linear regression outputs and true labels, compute\n",
        "    # the batch of squared error losses. This is *without* the ridge\n",
        "    # regression penalty.\n",
        "    #\n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n",
        "    #     of dimension d\n",
        "    #\n",
        "    # y := Tensor(int) of size (m,) --- This is a batch of m real-valued labels\n",
        "    #\n",
        "    # w := Tensor(float) of size(d,) --- This is the weights of a linear\n",
        "    #     classifer\n",
        "    #\n",
        "    # Return := Tensor of size (m,) --- This is the squared loss for each\n",
        "    #     example\n",
        "\n",
        "    # Fill in the rest\n",
        "    return torch.pow(y - torch.matmul(X,w), 2)\n",
        "\n",
        "def regression_fit(X, y, ridge_penalty=1.0):\n",
        "    # Given a dataset of examples and labels, fit the weights of the linear\n",
        "    # regression classifier using the provided loss function and optimizer\n",
        "    #\n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n",
        "    #     of dimension d\n",
        "    #\n",
        "    # y := Tensor(float) of size (m,) --- This is a batch of m real-valued\n",
        "    #     labels\n",
        "    #\n",
        "    # ridge_penalty := float --- This is the parameter for ridge regression\n",
        "    #\n",
        "    # Return := Tensor of size (d,) --- This is the fitted weights of the\n",
        "    #     linear regression model\n",
        "    #\n",
        "    # Fill in the rest\n",
        "    inner = torch.matmul(X.t(), X) + ridge_penalty*X.size(0)*torch.eye(X.size(1))\n",
        "    outer = torch.matmul(X.t(), y)\n",
        "    return torch.matmul(torch.inverse(inner), outer)\n",
        "\n",
        "def regression_predict(X, w):\n",
        "    # Given a dataset of examples and fitted weights for a linear regression\n",
        "    # classifier, predict the label\n",
        "    #\n",
        "    # X := Tensor(float) of size(m,d) --- This is a batch of m examples of\n",
        "    #    dimension d\n",
        "    #\n",
        "    # w := Tensor(float) of size (d,) --- This is the fitted weights of the\n",
        "    #    linear regression model\n",
        "    #\n",
        "    # Return := Tensor of size (m,) --- This is the predicted real-valued labels\n",
        "    #    for each example\n",
        "    #\n",
        "    # Fill in the rest\n",
        "    return torch.matmul(X,w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AplqTgtcWAhu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train accuracy [zero]: 32.28\n",
            "Test accuracy [zero]: 32.97\n",
            "Train accuracy [random]: 29.64\n",
            "Test accuracy [random]: 29.55\n",
            "Train accuracy [fitted]: 8.37\n",
            "Test accuracy [fitted]: 8.60\n"
          ]
        }
      ],
      "source": [
        "# Test your code on the wine dataset!\n",
        "# How does your solution compare to a random linear classifier?\n",
        "# Your solution should get an average squard error of about 8.6 test set.\n",
        "torch.manual_seed(42)\n",
        "\n",
        "d = X_train.size(1)\n",
        "regression_weights = {\n",
        "    'zero': torch.zeros(d),\n",
        "    'random': torch.randn(d),\n",
        "    'fitted': regression_fit(X_train, y_regression_train)\n",
        "}\n",
        "\n",
        "for k,w in regression_weights.items():\n",
        "    yp_regression_train = regression_predict(X_train, w)\n",
        "    squared_loss_train = regression_loss(X_train, y_regression_train, w).mean()\n",
        "\n",
        "    print(f'Train accuracy [{k}]: {squared_loss_train.item():.2f}')\n",
        "\n",
        "    yp_regression_test = regression_predict(X_test, w)\n",
        "    squared_loss_test = regression_loss(X_test, y_regression_test, w).mean()\n",
        "\n",
        "    print(f'Test accuracy [{k}]: {squared_loss_test.item():.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "e2DuhYO7WBlR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "grader.grade(test_case_id = 'regression_loss', answer = getsource(regression_loss))\n",
        "grader.grade(test_case_id = 'regression_fit', answer = getsource(regression_fit))\n",
        "grader.grade(test_case_id = 'regression_predict', answer = getsource(regression_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DkcvwEw01Du"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
