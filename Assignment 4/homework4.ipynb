{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1dkS0VksGp7"
   },
   "source": [
    "# CIS 5200: Machine Learning\n",
    "## Homework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "QXZxyZDbr-Yx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO, OK] Google Colab.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# For autograder only, do not modify this cell.\n",
    "# True for Google Colab, False for autograder\n",
    "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
    "if NOTEBOOK:\n",
    "    print(\"[INFO, OK] Google Colab.\")\n",
    "else:\n",
    "    print(\"[INFO, OK] Autograder.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swECpqQGvLu9"
   },
   "source": [
    "### Penngrader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "-peqcQNCvFSS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: penngrader-client in /opt/homebrew/Caskroom/miniconda/base/envs/pyenv/lib/python3.13/site-packages (0.5.2)\n",
      "Requirement already satisfied: dill in /opt/homebrew/Caskroom/miniconda/base/envs/pyenv/lib/python3.13/site-packages (from penngrader-client) (0.3.9)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniconda/base/envs/pyenv/lib/python3.13/site-packages (from penngrader-client) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install penngrader-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "9VOzgVapPgrZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
    "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "gSSxUlaHvsrK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PennGrader initialized with Student ID: 17994725\n",
      "\n",
      "Make sure this correct or we will not be able to store your grade\n"
     ]
    }
   ],
   "source": [
    "from penngrader.grader import PennGrader\n",
    "\n",
    "# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
    "# TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
    "STUDENT_ID = 17994725 # YOUR PENN-ID GOES HERE AS AN INTEGER #\n",
    "SECRET = STUDENT_ID\n",
    "\n",
    "grader = PennGrader('config.yaml', 'cis5200_sp25_HW4', STUDENT_ID, SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "SRSiFAHsu0UQ"
   },
   "outputs": [],
   "source": [
    "# packages for homework\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from inspect import getsource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7LWmjB_64uO"
   },
   "source": [
    "# 1. Boosting (5 pts)\n",
    "\n",
    "In this problem, you'll implement a basic boosting algorithm on the binary classification breast cancer dataset. Here, we've provided the following weak learner for you: an $\\ell_2$ regularized logistic classifier trained with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "ppRNBbiY84N5"
   },
   "outputs": [],
   "source": [
    "class Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Logistic, self).__init__()\n",
    "        self.linear = nn.Linear(30,1)\n",
    "    def forward(self, X):\n",
    "        out = self.linear(X)\n",
    "        return out.squeeze()\n",
    "\n",
    "def fit_logistic_clf(X,y):\n",
    "    clf = Logistic()\n",
    "    opt = torch.optim.Adam(clf.parameters(), lr=0.1, weight_decay=1e2)\n",
    "    loss = torch.nn.BCEWithLogitsLoss()\n",
    "    for t in range(200):\n",
    "        out = clf(X)\n",
    "        opt.zero_grad()\n",
    "        loss(out,(y>0).float()).backward()\n",
    "        # if t % 50 == 0:\n",
    "        #     print(loss(out,y.float()).item())\n",
    "        opt.step()\n",
    "    return clf\n",
    "\n",
    "def predict_logistic_clf(X, clf):\n",
    "    return torch.sign(clf(X)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVxoFHtpEXtx"
   },
   "source": [
    "Your task is to boost this logistic classifier to reduce its bias. Implement the following two functions:\n",
    "\n",
    "+ Finish the boosting algorithm: we've provided a template for the boosting algorithm in `boosting_fit`, however it is missing several components. Fill in the missing snippets of code.\n",
    "+ Prediction after boosting (2pts): implement `boosting_predict` to make predictions with a given boosted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "6EujmjUe_dQx"
   },
   "outputs": [],
   "source": [
    "def boosting_fit(X, y, T, fit_logistic_clf, predict_logistic_clf):\n",
    "    # X := Tensor(float) of size (m,d) -- Batch of m examples of demension d\n",
    "    # y := Tensor(int) of size (m) -- the given vectors of labels of the examples\n",
    "    # T := Maximum number of models to be implemented\n",
    "\n",
    "    m = X.size(0)\n",
    "    clfs = []\n",
    "    mu = torch.ones(m) / m\n",
    "\n",
    "    while len(clfs) < T:\n",
    "        # Calculate the weights for each sample mu. You may need to\n",
    "        # divide this into the base case and the inductive case.\n",
    "\n",
    "        ## ANSWER\n",
    "        mu /= mu.sum()\n",
    "        ## END ANSWER\n",
    "\n",
    "        # Here, we draw samples according to mu and fit a weak classifier\n",
    "        idx = torch.multinomial(mu, m, replacement=True)\n",
    "        X0, y0 = X[idx], y[idx]\n",
    "\n",
    "        clf = fit_logistic_clf(X0, y0)\n",
    "\n",
    "        # Calculate the epsilon error term\n",
    "\n",
    "        ## ANSWER\n",
    "        prediction = predict_logistic_clf(X, clf)\n",
    "        eps = torch.sum(mu * (prediction != y))\n",
    "        ## END ANSWER\n",
    "\n",
    "        if eps > 0.5:\n",
    "            # In the unlikely even that gradient descent fails to\n",
    "            # find a good classifier, we'll skip this one and try again\n",
    "            continue\n",
    "\n",
    "        # Calculate the alpha term here\n",
    "\n",
    "        ## ANSWER\n",
    "        alpha = torch.log2((1 - eps) / eps) / 2\n",
    "        mu *= torch.exp(-alpha * y * prediction)\n",
    "        mu /= mu.sum()\n",
    "        ## END ANSWER\n",
    "\n",
    "        clfs.append((alpha,clf))\n",
    "    return clfs\n",
    "\n",
    "def boosting_predict(X, clfs, predict_logistic_clf):\n",
    "    # X := Tensor(float) of size (m,d) -- Batch of m examples of demension d\n",
    "    # clfs := list of tuples of (float, logistic classifier) -- the list of boosted classifiers\n",
    "    # Return := Tnesor(int) of size (m) -- the predicted labels of the dataset\n",
    "\n",
    "    output = torch.zeros(X.size(0))\n",
    "    for alpha, clf in clfs:\n",
    "        output += alpha * predict_logistic_clf(X, clf)\n",
    "    return torch.sign(output).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0u_Yx5qAo9D"
   },
   "source": [
    "Test out your code on the breast cancer dataset. As a sanity check, your statndard logistic classifier will get a train/test accuracy of around 80% while the boosted logistic classifier will get a train/test accuracy of around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "YoT4dRVBDjsy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic classifier accuracy:\n",
      "Train accuracy:  0.8021978139877319\n",
      "Test accuracy:  0.8333333134651184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosted logistic classifier accuracy:\n",
      "Train accuracy:  0.9076923131942749\n",
      "Test accuracy:  0.9385964870452881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = datasets.load_breast_cancer()\n",
    "data=train_test_split(cancer.data,cancer.target,test_size=0.2,random_state=123)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "X,X_te,y,y_te = [torch.from_numpy(A) for A in data]\n",
    "X,X_te,y,y_te = X.float(), X_te.float(), torch.sign(y.long()-0.5), torch.sign(y_te.long()-0.5)\n",
    "\n",
    "\n",
    "logistic_clf = fit_logistic_clf(X,y)\n",
    "print(\"Logistic classifier accuracy:\")\n",
    "print('Train accuracy: ', (predict_logistic_clf(X, logistic_clf) == y).float().mean().item())\n",
    "print('Test accuracy: ', (predict_logistic_clf(X_te, logistic_clf) == y_te).float().mean().item())\n",
    "\n",
    "boosting_clfs = boosting_fit(X,y, 10, fit_logistic_clf, predict_logistic_clf)\n",
    "print(\"Boosted logistic classifier accuracy:\")\n",
    "print('Train accuracy: ', (boosting_predict(X, boosting_clfs, predict_logistic_clf) == y).float().mean().item())\n",
    "print('Test accuracy: ', (boosting_predict(X_te, boosting_clfs, predict_logistic_clf) == y_te).float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRjBW3CpAymG"
   },
   "source": [
    "## Autograder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "vjZGZ_cyAv2k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 3/3 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n",
      "Correct! You earned 2/2 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "grader.grade(test_case_id = 'boosting_fit', answer = getsource(boosting_fit))\n",
    "grader.grade(test_case_id = 'boosting_predict', answer = getsource(boosting_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmyVGWOfPkS_"
   },
   "source": [
    "# 2. Extending Reverse Mode Auto-differentiation (8 pts)\n",
    "\n",
    "In lecture we learned about how auto-differentiation could be used to automatically calculate gradients through a computational graph. Crucially, all we needed to do was know how to propagate variations of the chain rule at each individual node in the graph.\n",
    "\n",
    "The PyTorch framework makes it very simple to extend auto-differentation to new formula or expressions. Specifically, PyTorch keeps track of the computational graph with the `forward` function, saving relevant computations, and then computes the chain rule with the `backward` function (in reverse mode).\n",
    "\n",
    "For example, consider the Legendre polynomial of degree 3, $P_3(x) = \\frac{1}{2}(5x^3 - 3x)$. How would we implement our own custom module to do this, if it didn't already exist in PyTorch? We can do this like in the following ([example taken from the PyTorch documetnation](https://pytorch.org/docs/stable/notes/extending.html)):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "wEjxtGuvPk38"
   },
   "outputs": [],
   "source": [
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsFCXkjyPn_Z"
   },
   "source": [
    "And that's it! We can now use the `LegendrePolynomial3` function in combination with any other PyTorch functions and automatically calculate gradients with auto-differentiation. Note that, since PyTorch uses *reverse-mode*, in the `backward` function we are:  \n",
    "1. Given the gradient of the loss with respect to the output of the function, and\n",
    "2. Need to recompute the gradient of the loss with respect to the inputs of the function.\n",
    "\n",
    "This is an extremely general and powerful framework. For example, researchers have implemented forward functions and their gradients for modules as complex as a call to an external simulator such as a physics engine, which would then let you differentiate through the simulator!\n",
    "\n",
    "In this exercise, you'll implement a new PyTorch function for a simpler function, the integral:\n",
    "$$f(x) = \\int_{-\\infty}^x g(t)dt$$\n",
    "PyTorch doesn't have a function for doing numerical integration, so we'll need to implement the backwards function ourselves if we want to use auto-differentiate with integrals. In particular, we'll do this for the following piece-wise constant function:\n",
    "\n",
    "$$g(x; \\beta, \\eta) = \\sum_{r=1}^R \\beta_r \\mathbb 1[x\\in (\\eta_{r-1},\\eta_r)]$$\n",
    "\n",
    "for parameters $\\beta\\in \\mathbb R^R$ and $\\eta\\in \\mathbb R^{R+1}$.\n",
    "\n",
    "1. Forward (2pts): Implement the `forward` function which calculates $f(x)$ for a batch of example $x$.\n",
    "2. Backward (6pts): Implement the `backward` function which calculates the reverse-mode auto-differentiation rule for $f$ with respect to $x,\\beta, \\eta$, 2 points each.\n",
    "\n",
    "Hints:\n",
    "+ Recall that $g(x)$ is a piece-wise constant function, and the integral of a function is the area between the function and 0. Thus, you can reformulate the integral of each constant part of $g$ as simply the (signed) area of the rectangle, and so the integral of $g(x)$ is the sum of the areas of all the piece-wise rectangles up to $x$\n",
    "+ Recall that for reverse-mode autodifferentiation, we are given the derivative of the loss with respect to the output, or $\\frac{\\partial \\ell(x))}{\\partial f(x; \\beta, \\eta)}$, and our goal is to then compute $\\frac{\\partial \\ell(x))}{\\partial x}, \\frac{\\partial \\ell(x))}{\\partial \\beta}, \\frac{\\partial \\ell(x))}{\\partial \\eta}$.\n",
    "+ Remember the chain rule: $$\\frac{\\partial \\ell(x))}{\\partial x} = \\sum_i \\frac{\\partial \\ell(x))}{\\partial f(x; \\beta, \\eta)_i} \\cdot \\frac{\\partial f(x; \\beta, \\eta))_i}{\\partial x}$$\n",
    "+ For simplicity, you can assume that all $x$'s and $\\eta$'s are distinct so you don't have to worry about border cases for the intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "f4tNbruOPnwR"
   },
   "outputs": [],
   "source": [
    "class IntegralPiecewiseConstant(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, beta, eta):\n",
    "        # ctx := A PyTorch object for saving data for use in the backward\n",
    "        #   function\n",
    "        # X := Tensor(float) of size (m) -- a minibatch of examples\n",
    "        # beta := Tensor(float) of size (k) -- the magnitudes of each part of\n",
    "        #   the piece-wise constant function\n",
    "        # eta := Tensor(float) of size (k+1) -- the start/end points of each\n",
    "        #   part of the piece-wise constant function\n",
    "        # Return the integral of the piece-wise constant function applied to\n",
    "        #   each entry of  X\n",
    "\n",
    "        # Here we save the inputs to the forward function to use in backward\n",
    "        ctx.save_for_backward(X, beta, eta)\n",
    "\n",
    "        # Fill in the rest\n",
    "        m = X.size(0)\n",
    "        n = beta.size(0)\n",
    "        ans = torch.zeros(m)\n",
    "\n",
    "        for i in range(m):\n",
    "            x = X[i]\n",
    "            xi_int = 0\n",
    "            for j in range(n):\n",
    "                if x > eta[j]:\n",
    "                    xi_int += beta[j] * (min(x, eta[j+1]) - eta[j])\n",
    "            ans[i] = xi_int\n",
    "\n",
    "        return ans\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # ctx := A PyTorch object for containing saved data from the forward\n",
    "        #   function\n",
    "        # grad_output := Tensor(float) of size (m) -- a minibatch of gradients\n",
    "        #   of the loss with respect to the output of this function. Since we\n",
    "        #   are working with a scalar function, each element is the gradient\n",
    "        #   with respect to an output of the minibatch. In other words, this is\n",
    "        #   dL/df(x) where this module outputs f(x).\n",
    "        # Return a tuple containing the gradient of the loss with respect to\n",
    "        #   the inputs X, beta, and eta: (dL/dX, dL/dbeta, dL/deta).\n",
    "\n",
    "        # Here we retrieve the tensors from the forward pass\n",
    "        X, beta, eta = ctx.saved_tensors\n",
    "        print(grad_output)\n",
    "\n",
    "        # Fill in the rest\n",
    "        m = X.size(0)\n",
    "        n = beta.size(0)\n",
    "        p = eta.size(0)\n",
    "\n",
    "        dX = torch.zeros(m)\n",
    "        dbeta = torch.zeros(n)\n",
    "        deta = torch.zeros(p)\n",
    "\n",
    "        for i in range(m):\n",
    "            x  = X[i]\n",
    "            gradient = grad_output[i]\n",
    "            for j in range(n):\n",
    "                if eta[j] < x and x <= eta[j + 1]:\n",
    "                    dX[i] += gradient * beta[j]\n",
    "            for j in range(n):\n",
    "                if x > eta[j]:\n",
    "                    dbeta[j] += gradient * (min(x, eta[j + 1]) - eta[j])\n",
    "            for j in range(n):\n",
    "                if x > eta[j]:\n",
    "                    deta[j] -= beta[j] * gradient\n",
    "                if x > eta[j + 1]:\n",
    "                    deta[j + 1] += beta[j] * gradient\n",
    "\n",
    "        return dX, dbeta, deta    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KB7NXfAMPtcJ"
   },
   "source": [
    "As an example, calculating the gradients for the following example will result in the following output:\n",
    "```\n",
    "tensor([ 1., -1.,  3.])\n",
    "tensor([0.2500, 0.4000, 0.7000, 0.1000])\n",
    "tensor([-3., -2.,  6., -4.,  0.])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "uqlAqg6LPu6Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([ 1., -1.,  3.])\n",
      "tensor([0.2500, 0.4000, 0.7000, 0.1000])\n",
      "tensor([-3., -2.,  6., -4.,  0.])\n"
     ]
    }
   ],
   "source": [
    "X = torch.Tensor([0.05,0.5,0.9])\n",
    "betas = torch.Tensor([1,2,-1,3])\n",
    "etas = torch.Tensor([0,0.1,0.3,0.8,1])\n",
    "\n",
    "X.requires_grad = True\n",
    "betas.requires_grad = True\n",
    "etas.requires_grad = True\n",
    "IntegralPiecewiseConstant.apply(X, betas, etas).sum().backward()\n",
    "\n",
    "print(X.grad)\n",
    "print(betas.grad)\n",
    "print(etas.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "IFVintChPz0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 2/2 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n",
      "Correct! You earned 6/6 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "IPC_str = getsource(IntegralPiecewiseConstant.forward) + getsource(IntegralPiecewiseConstant.backward)\n",
    "grader.grade(test_case_id = 'autodiff_forward', answer = IPC_str)\n",
    "grader.grade(test_case_id = 'autodiff_backward', answer = IPC_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82AEAjkDP8Vm"
   },
   "source": [
    "# 3. Neural Networks and Gradient Descent (5 pts)\n",
    "\n",
    "In the previous example, we directly calculated the gradient of a function with respect to various inputs using PyTorch's `autograd` library. As we did in that problem, one can use this autograd library to directly implement gradient descent by iterating over all parameters and applying the gradient update. However, as the number of parameters grow, directly implementing these updates can become quite onerous. To handle neural networks with lots of parameters, the PyTorch library includes optimizers that make training with gradient descent very easy with the following 5 steps:\n",
    "1. Create an optimizer object and give it all the parameters you'd like to optimize\n",
    "2. Calculate a loss that you'd like to minimize\n",
    "3. Clear old gradients\n",
    "4. Calculate new gradients\n",
    "5. Update the parameters with one gradient step\n",
    "\n",
    "The end result is a generic boilerplate recipe that will optimize *any* objective with gradient descent. Here is an example running gradient descent on a linear model, using the stochastic gradient descent (SGD) optimizer and a basic dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "0pnA6QH8AKBo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.597830057144165\n",
      "1.0670807361602783\n",
      "0.717989444732666\n",
      "0.487011581659317\n",
      "0.33319732546806335\n",
      "0.23005399107933044\n",
      "0.16037000715732574\n",
      "0.11291281878948212\n",
      "0.08031636476516724\n",
      "0.0577247180044651\n"
     ]
    }
   ],
   "source": [
    "# Setup a simple problem\n",
    "m,d = 128,5\n",
    "X = torch.randn(m,d)\n",
    "w_opt, b_opt = torch.randn(d), torch.randn(1)\n",
    "y = X.matmul(w_opt) + b_opt\n",
    "\n",
    "# setup the dataloader\n",
    "simple_dataset = torch.utils.data.TensorDataset(X,y)\n",
    "loader = torch.utils.data.DataLoader(simple_dataset,batch_size=16)\n",
    "\n",
    "# Create the model\n",
    "lin = nn.Linear(d,1)\n",
    "\n",
    "# setup the optimizer (Step 1)\n",
    "opt = torch.optim.SGD(lin.parameters(), lr=0.001)\n",
    "\n",
    "# iterate over epochs\n",
    "for i in range(100):\n",
    "    # iterate over minibatches\n",
    "    for X0,y0 in loader:\n",
    "        yhat = lin(X0).squeeze(1) # make predictions\n",
    "        loss = F.mse_loss(yhat,y0) # calculate loss (Step 2)\n",
    "\n",
    "        opt.zero_grad() # clear gradients from previous iteration (Step 3)\n",
    "        loss.backward() # calculate new gradients (Step 4)\n",
    "        opt.step() # update parameters (Step 5)\n",
    "\n",
    "    # logging\n",
    "    with torch.no_grad():\n",
    "        if i % 10 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL9ycJUKP_t3"
   },
   "source": [
    "\n",
    "\n",
    "In the second part of this assignment, we'll implement a basic neural network for a tree cover classification problem using the PyTorch library. Here, the problem is to use 12 features (which have been expanded to 54 columns of data to expand categorical variables into binary features) to predict one of 7 tree cover types. A full description of the dataset can be found [here](http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info). The following cell will download the data and convert it into PyTorch tensors for you, with a training split of 2000 examples per class and a validation split of 500 examples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "hVVKICOHTEfi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /opt/homebrew/Caskroom/miniconda/base/envs/pyenv/lib/python3.13/site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/pyenv/lib/python3.13/site-packages (from ucimlrepo) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /opt/homebrew/Caskroom/miniconda/base/envs/pyenv/lib/python3.13/site-packages (from ucimlrepo) (2025.1.31)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/pyenv/lib/python3.13/site-packages (from pandas>=1.0.0->ucimlrepo) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/pyenv/lib/python3.13/site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/pyenv/lib/python3.13/site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/pyenv/lib/python3.13/site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/pyenv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ucimlrepo\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "covertype = fetch_ucirepo(id=31)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = covertype.data.features\n",
    "y = covertype.data.targets\n",
    "\n",
    "# metadata\n",
    "# print(covertype.metadata)\n",
    "\n",
    "# variable information\n",
    "# print(covertype.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "jB6JPHP4T-hP"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([covertype.data.features, covertype.data.targets], axis = 1)\n",
    "df = pd.concat([df.iloc[df.values[:,-1]==i].sample(2500) for i in range(1,8)])\n",
    "X,y = df.values[:,:-1], df.values[:,-1]-1 # re-index labels to 0-6\n",
    "X,y = [torch.from_numpy(a) for a in (X,y)] # convert to PyTorch\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X,y)\n",
    "train_set,val_set = torch.utils.data.random_split(dataset,[2000*7,500*7]) # generate splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WHjW_JdQGNf"
   },
   "source": [
    "Your goal is to achieve at least 70% accuracy on this forest cover task. We suggest a very simple neural network.\n",
    "\n",
    "1. (5pts) Implement a model for predicting forest cover. You will get 1 point for every first 14% accuracy, up to 70% for a total of 5 points. This is achievable with a small neural network with one hidden layer and ReLU activations. It is possible to achieve over 80% accuracy with less than 15 seconds (many solutions exist that take less than a minute).\n",
    "\n",
    "It is possible to get much higher than 75%, in fact it is possible to exceed 80% accuracy. **You do not need a GPU**.\n",
    "\n",
    "Hints:\n",
    "+ A simple neural network directly on the data can get around 50% accuracy. You can start with a sequential model, linear layers, and ReLU activations (PyTorch has a wide range of modules [here](https://pytorch.org/docs/stable/nn.html)).\n",
    "+ To iterate over minibatches, PyTorch has a useful `DataLoader` object with documentation [here](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader).\n",
    "+ Data normalization can be a very important factor, not just in deep learning. Check your dataset statistics and normalize to improve your accuracy further.\n",
    "+ You may need to explore different types of optimizers and learning rates.  \n",
    "+ Remember to check your performance on the validation set, and not the training set. You will need to perform well on the held-out test data.\n",
    "+ To help with your testing, you can add additional parameters via keyword arguments to tweak your pipeline.\n",
    "+ There is no single right answer here---the only goal is to get to 70%!\n",
    "+ Solutions that exploit the grader to extract test set labels will receive no credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "UtzdW42oQHp6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "class ForestCover():\n",
    "    def __init__(self, train_set, nhidden=100):\n",
    "        # train_set := a PyTorch dataset of training examples from the tree\n",
    "        #   cover prediction problem.\n",
    "\n",
    "        # Initialize your model here!\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(train_set[0][0].shape[0], nhidden),\n",
    "            nn.ReLU(),\n",
    "             nn.Linear(nhidden, int(nhidden/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(nhidden/2), 7),\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        inputs = torch.stack([x for x, _ in train_set])\n",
    "        inputs = inputs.float()\n",
    "        self.mean = inputs.mean(dim=0)\n",
    "        self.std = inputs.std(dim=0)\n",
    "\n",
    "    def train(self, train_set, epochs=20):\n",
    "        # train_set := a PyTorch dataset of training examples from the tree\n",
    "        #   cover prediction problem.\n",
    "\n",
    "        # Train your model here!\n",
    "        train_data = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for X, y in train_data:\n",
    "                std = torch.where(self.std == 0, torch.ones_like(self.std), self.std)\n",
    "                X = (X.float() - self.mean) / std                 \n",
    "\n",
    "                output = self.model(X)\n",
    "                batch_loss = self.loss_function(output, y.long())\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += batch_loss.item()\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # X := a Tensor(float) of size (m,d) of examples for the model to\n",
    "        #   to predict.\n",
    "\n",
    "        # Make predictions on a new input here!\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X = (X.float() - self.mean) / torch.where(self.std == 0, torch.ones_like(self.std), self.std)\n",
    "            output = self.model(X)\n",
    "            prediction = torch.argmax(output, dim=1)\n",
    "        return prediction\n",
    "\n",
    "solution = ForestCover(train_set)\n",
    "solution.train(train_set)\n",
    "\n",
    "acc = []\n",
    "for X,y in torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False):\n",
    "    y_pred = solution.predict(X)\n",
    "    acc.append(y_pred == y)\n",
    "\n",
    "acc = torch.cat(acc).float().mean()\n",
    "print(f\"Validation accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LO5DHi9OMkTB"
   },
   "source": [
    "Your code will be scored according to accuracy on a held-out test set.\n",
    "If you don't use your validation set during training, your performance on the validation set will be approximately your performance on the test set.\n",
    "**Warning: solutions that exploit the grader to extract test set labels will receive a manual adjustment for zero credit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "d_XlEvpP4WC3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-14 20:24:56--  https://machine-learning-upenn.github.io/assets/hw3/X_test.pth\n",
      "Resolving machine-learning-upenn.github.io (machine-learning-upenn.github.io)... 2606:50c0:8003::153, 2606:50c0:8002::153, 2606:50c0:8001::153, ...\n",
      "Connecting to machine-learning-upenn.github.io (machine-learning-upenn.github.io)|2606:50c0:8003::153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 303511 (296K) [application/octet-stream]\n",
      "Saving to: ‘X_test.pth’\n",
      "\n",
      "X_test.pth          100%[===================>] 296.40K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-04-14 20:24:56 (9.27 MB/s) - ‘X_test.pth’ saved [303511/303511]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://machine-learning-upenn.github.io/assets/hw3/X_test.pth -O \"X_test.pth\"\n",
    "X_test = torch.load(\"X_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "0gdUbvJ3HH7y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 5/5 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "y_soln = solution.predict(X_test)\n",
    "grader.grade(test_case_id = 'forestcover', answer = y_soln.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqO2yBcoHw3A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1cTyaVVfbNpflia0WlLbSDy778oSU3D7L",
     "timestamp": 1710354670803
    }
   ]
  },
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
