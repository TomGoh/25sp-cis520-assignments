{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1dkS0VksGp7"
   },
   "source": [
    "# CIS 5200: Machine Learning\n",
    "## Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QXZxyZDbr-Yx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO, OK] Google Colab.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# For autograder only, do not modify this cell.\n",
    "# True for Google Colab, False for autograder\n",
    "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
    "if NOTEBOOK:\n",
    "    print(\"[INFO, OK] Google Colab.\")\n",
    "else:\n",
    "    print(\"[INFO, OK] Autograder.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swECpqQGvLu9"
   },
   "source": [
    "### Penngrader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-peqcQNCvFSS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: penngrader-client in /opt/homebrew/Caskroom/miniconda/base/envs/mlenv/lib/python3.12/site-packages (0.5.2)\n",
      "Requirement already satisfied: dill in /opt/homebrew/Caskroom/miniconda/base/envs/mlenv/lib/python3.12/site-packages (from penngrader-client) (0.3.9)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniconda/base/envs/mlenv/lib/python3.12/site-packages (from penngrader-client) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install penngrader-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9VOzgVapPgrZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
    "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SRSiFAHsu0UQ"
   },
   "outputs": [],
   "source": [
    "# packages for homework\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Dh5DoTTiORqw"
   },
   "outputs": [],
   "source": [
    "def get_class_source(cls):\n",
    "    import re\n",
    "    class_name = cls.__name__\n",
    "    from IPython import get_ipython\n",
    "    ipython = get_ipython()\n",
    "    inputs = ipython.user_ns['In']\n",
    "    pattern = re.compile(r'^\\s*class\\s+{}\\b'.format(class_name))\n",
    "    for cell in reversed(inputs):\n",
    "        if pattern.search(cell):\n",
    "            return cell\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SWSEcFTlOVI4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PennGrader initialized with Student ID: 17994725\n",
      "\n",
      "Make sure this correct or we will not be able to store your grade\n"
     ]
    }
   ],
   "source": [
    "# Autograder will be announced on Ed Discussion approximately a week after initial release\n",
    "from penngrader.grader import PennGrader\n",
    "from dill.source import getsource\n",
    "\n",
    "# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
    "# TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
    "STUDENT_ID = 17994725 # YOUR PENN-ID GOES HERE AS AN INTEGER #\n",
    "SECRET = STUDENT_ID\n",
    "\n",
    "grader = PennGrader('config.yaml', 'cis5200_sp25_HW3', STUDENT_ID, SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UaAal2PnSUH"
   },
   "source": [
    "### 0. Gradients with PyTorch\n",
    "\n",
    "At this point, you've implemented a lot of gradients. However, these days, manual implementation of gradients is a thing of the past: PyTorch is a scientific computing library that comes with the ability to automatically compute gradients for you! This is called auto-differentiation. Here is an example of using auto-differentiation to compute the gradient of a quadratic function, $f(x) = ax^2$. The key parts are as follows:\n",
    "\n",
    "1. Variables that you want to differentiate with respect to should have the `requires_grad` flag set to `True`.\n",
    "2. Calculate the objective that you'd like the compute the gradient of, using the variable from step (1).\n",
    "3. Pass the objective and the variable you are differentiating to `torch.autograd.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Rl2V8wB0nT-B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch gradient: tensor([9.])\n",
      "Analytic gradient: tensor([9.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set requires_grad to true for x\n",
    "x = torch.Tensor([3.0])\n",
    "a = torch.Tensor([1.5])\n",
    "x.requires_grad = True\n",
    "\n",
    "# Step 2: Compute the objective\n",
    "y = a*(x**2)\n",
    "\n",
    "# Step 3: Use autograd\n",
    "grad = torch.autograd.grad([y],[x])[0]\n",
    "\n",
    "print(\"PyTorch gradient:\", grad)\n",
    "print(\"Analytic gradient:\", 2*a*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voRzKdQpnVm6"
   },
   "source": [
    "You'll notice that the gradient computed with PyTorch matches exactly the analytic gradient $\\nabla f(x) = 2ax$, but without having to implement or derive the analytic gradient! This works for gradients with respect to any sized variables. For example, if $x$ is now a vector, and the objective is $f(x) = a\\|x\\|_2^2$ then we can calculate the gradient in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FHQZ4KEEnW4T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch gradient: tensor([9.0000, 6.0000])\n",
      "Analytic gradient: tensor([9., 6.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set requires_grad to true for x\n",
    "x = torch.Tensor([3.0, 2.0])\n",
    "a = torch.Tensor([1.5])\n",
    "x.requires_grad = True\n",
    "\n",
    "# Step 2: Compute the objective\n",
    "y = a*(x.norm(p=2)**2)\n",
    "\n",
    "# Step 3: Use autograd\n",
    "grad = torch.autograd.grad([y],[x])[0]\n",
    "\n",
    "print(\"PyTorch gradient:\", grad)\n",
    "print(\"Analytic gradient:\", 2*a*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvbUL-AonZDp"
   },
   "source": [
    "From now on, we highly recommend that you use auto-differentiation to calculate gradients. As long as all of your operations are differentiable, the final objective will be differentiable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV31k5WPnbtV"
   },
   "source": [
    "# 1. SVM and Gradient Descent\n",
    "\n",
    "In this first problem, you'll implement (soft margin) support vector machines with gradient descent, using gradients from PyTorch's autodifferentiation library.\n",
    "+ (2pts) Calculate the objective of the Soft SVM\n",
    "+ (2pts) Calculate the gradient of the Soft SVM objective\n",
    "+ (2pts) Implement a basic gradient descent optimizer. Your solution needs to converge to an accurate enough answer.\n",
    "+ (1pts) Make predictions with the Soft SVM\n",
    "\n",
    "Tips:\n",
    "- This assignment is more freeform than previous ones. You're allowed to initialize the parameters of the SVM model however you want, as long as your implemented functions return the right values.\n",
    "- We recommend using PyTorch's `torch.autograd.grad` to get the gradient instead of deriving the SVM gradient.\n",
    "- You'll need to play with the values of step size and number of iterations to\n",
    "converge to a good value.\n",
    "- To debug your optimization, print the objective over iterations. Remember that the theory says as long as the learning rate is small enough, for strongly convex problems, we are guaranteed to converge at a certain rate. What does this imply about your solution if it is not converging?\n",
    "- As a sanity check, you can get around 97.5% prediction accuracy and converge to an objective below 0.16.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WkblDNT5nePl"
   },
   "outputs": [],
   "source": [
    "class SoftSVM():\n",
    "    def __init__(self, ndims):\n",
    "        # Here, we initialize the parameters of your soft-SVM model for binary\n",
    "        # classification. You can change the initialization but don't change\n",
    "        # the weight and bias variables as the autograder will assume that\n",
    "        # these exist.\n",
    "        # ndims := integer -- number of dimensions\n",
    "        # no return type\n",
    "\n",
    "        self.weight = torch.randn(ndims)\n",
    "        self.bias = torch.randn(1)\n",
    "        self.weight.requires_grad = True\n",
    "        self.bias.requires_grad = True\n",
    "\n",
    "    def objective(self, X, y, l2_reg):\n",
    "        # Calculate the objective of your soft-SVM model\n",
    "        # X := Tensor of size (m,d) -- the input features of m examples with d dimensions\n",
    "        # y := Tensor of size (m) -- the labels for each example in X\n",
    "        # l2_reg := float -- L2 regularization penalty\n",
    "        # Returns a scalar tensor (zero dimensional tensor) -- the loss for the model\n",
    "        # Fill in the rest\n",
    "        predicted = torch.matmul(X, self.weight) + self.bias\n",
    "        hinge_loss = torch.mean(torch.max(torch.zeros_like(y), 1 - y*predicted))\n",
    "        l2_loss = l2_reg*torch.sum(self.weight**2)\n",
    "        return hinge_loss + l2_loss\n",
    "\n",
    "\n",
    "    def gradient(self, X, y, l2_reg):\n",
    "        # Calculate the gradient of your soft-SVM model\n",
    "        # X := Tensor of size (m,d) -- the input features of m examples with d dimensions\n",
    "        # y := Tensor of size (m) -- the labels for each example in X\n",
    "        # l2_reg := float -- L2 regularization penalty\n",
    "        # Return Tuple (Tensor, Tensor) -- the tensors corresponds to the\n",
    "        # gradients of the weight and bias parameters respectively\n",
    "        # Fill in the rest\n",
    "        objective = self.objective(X, y, l2_reg)\n",
    "        grad = torch.autograd.grad(objective, [self.weight, self.bias])\n",
    "        weight_grad = grad[0]\n",
    "        bias_grad = grad[1]\n",
    "        return weight_grad, bias_grad\n",
    "\n",
    "\n",
    "    def optimize(self, X, y, l2_reg, learning_rate=0.01):\n",
    "        # Calculate the gradient of your soft-SVM model\n",
    "        # X := Tensor of size (m,d) -- the input features of m examples with d dimensions\n",
    "        # y := Tensor of size (m) -- the labels for each example in X\n",
    "        # l2_reg := float -- L2 regularization penalty\n",
    "\n",
    "        # no return type\n",
    "\n",
    "        # Fill in the rest\n",
    "        max_iterations = 1000\n",
    "        threshold = 1e-5\n",
    "        \n",
    "        prev_objective = float('inf')\n",
    "        \n",
    "        for i in range(max_iterations):\n",
    "            # Compute current objective\n",
    "            objective = self.objective(X, y, l2_reg).item()\n",
    "            \n",
    "            # Check for convergence\n",
    "            if abs(prev_objective - objective) < threshold:\n",
    "                break\n",
    "                \n",
    "            # Compute gradients\n",
    "            weight_grad, bias_grad = self.gradient(X, y, l2_reg)\n",
    "            \n",
    "            # Update parameters in-place\n",
    "            with torch.no_grad():\n",
    "                self.weight.data -= learning_rate * weight_grad\n",
    "                self.bias.data -= learning_rate * bias_grad\n",
    "                \n",
    "            prev_objective = objective\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        # Given an X, make a prediction with the SVM\n",
    "        # X := Tensor of size (m,d) -- features of m examples with d dimensions\n",
    "        # Return a tensor of size (m) -- the prediction labels on the dataset X\n",
    "\n",
    "        # Fill in the rest\n",
    "        return torch.sign(torch.matmul(X, self.weight) + self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2imUax-nhjx"
   },
   "source": [
    "Test the Soft SVM on the breast cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UkSrBIa9niqs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([569, 30]) torch.Size([569])\n",
      "\n",
      "Soft SVM objective: \n",
      "0.2088182121515274\n",
      "\n",
      "Soft SVM accuracy: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9771528840065002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X,y = torch.from_numpy(cancer['data']), torch.from_numpy(cancer['target'])\n",
    "mu,sigma = X.mean(0,keepdim=True), X.std(0,keepdim=True)\n",
    "X,y = ((X-mu)/sigma).float(),(y - 0.5).sign() # prepare data\n",
    "l2_reg = 0.1\n",
    "print(X.size(), y.size())\n",
    "\n",
    "# Optimize the soft-SVM with gradient descent\n",
    "clf = SoftSVM(X.size(1))\n",
    "clf.optimize(X,y,l2_reg)\n",
    "print(\"\\nSoft SVM objective: \")\n",
    "print(clf.objective(X,y,l2_reg).item())\n",
    "print(\"\\nSoft SVM accuracy: \")\n",
    "(clf.predict(X) == y).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KYqv4JQLnkWT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 2/2 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n",
      "Correct! You earned 2/2 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n",
      "Correct! You earned 2/2 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n",
      "Correct! You earned 1/1 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "grader.grade(test_case_id = 'SVM_objective', answer = get_class_source(SoftSVM))\n",
    "grader.grade(test_case_id = 'SVM_gradient', answer = get_class_source(SoftSVM))\n",
    "grader.grade(test_case_id = 'SVM_optimize', answer = get_class_source(SoftSVM))\n",
    "grader.grade(test_case_id = 'SVM_predict', answer = get_class_source(SoftSVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yK9J863mtwKz"
   },
   "source": [
    "### 2. Decision Trees and Bagging\n",
    "\n",
    "In this problem, we'll implement a simplified version of random forests. We'll be using the `iris` dataset, which has 4 features that are discretized to $0.1$ steps between $0$ and $8$ (i.e. the set of all possible features is $\\{0.1, 0.2, \\dots, 7.8, 7.9\\}$. Thus, all thresholds that we'll need to consider are $\\{0.15, 0.25, \\dots, 7.75, 7.85\\}$.\n",
    "\n",
    "Your task in this first part is to finish the implementation of decision trees. We've provided a template for some of the decision tree code, following which you'll finish the bagging algorithm to get a random forest.\n",
    "\n",
    "1. Entropy (2pts): calculate the entropy of a given vector of labels in the `entropy` function. Note that the generalization of entropy to 3 classes is $H = -\\sum_{i=1}^3 p_i\\log(p_i)$ where $p_i$ is the proportion of examples with label $i$.\n",
    "2. Find the best split (1pt): finish the `find_split` function by finding the feature index and value that results in the split minimizing entropy.\n",
    "3. Build the tree (2pts): finish the `expand_node` function by completing the recursive call for building the decision tree.\n",
    "4. Predict with tree (2pts): implement the `predict_one` function, which makes a prediction for a single example.\n",
    "\n",
    "Throughout these problems, the way we represent the decision tree is by using python dicts. In particular, a node is a `dict` that can have the following keys:\n",
    "\n",
    "1. `node['label']` Return the label that we should predict upon reaching this node. node should **only** have a `label` entry if it is a leaf node.\n",
    "2. `node['left']` points to another node dict that represents this node's left child.\n",
    "3. `node['right']` points to another node dict that represents this node's right child.\n",
    "4. `node['split']` is a tuple containing the feature index and value that this node splits left versus right on.\n",
    "\n",
    "In our implementation, all comparisons will be **greater than** comparisons, and \"yes\" answers go **left**. In other words, if `node['split'] = (2, 1.25)`, then we expect to find all data remaining at this node with feature 2 value **greater** than 1.25 in `node['left']`, and feature value **less** than 1.25 in `node['right']`.\n",
    "\n",
    "Tips:\n",
    "+ If you have NaNs, you may be dividing by zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nko0MuabLNTW"
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "  def entropy(self, y):\n",
    "      # Calculate the entropy of a given vector of labels in the `entropy` function.\n",
    "      #\n",
    "      # y := Tensor(int) of size (m) -- the given vector of labels\n",
    "      # Return a float that is your calculated entropy\n",
    "\n",
    "      # Fill in the rest\n",
    "      pass\n",
    "\n",
    "  def find_split(self, node, X, y, k=4):\n",
    "      # Find the best split over all possible splits that minimizes entropy.\n",
    "      #\n",
    "      # node := Map(string: value) -- the tree represented as a Map, the key will take four\n",
    "      #   different string: 'label', 'split','left','right' (See implementation below)\n",
    "      #   'label': a label node with value as the mode of the labels\n",
    "      #   'split': the best split node with value as a tuple of feature id and threshold\n",
    "      #   'left','right': the left and right branch with value as the label node\n",
    "      # X := Tensor of size (m,d) -- Batch of m examples of demension d\n",
    "      # y := Tensor(int) of size (m) -- the given vectors of labels of the examples\n",
    "      # k := int -- the number of classes, with default value as 4\n",
    "      # Return := tuple of (int, float) -- the feature id and threshold of the best split\n",
    "\n",
    "      m = y.size(0)\n",
    "      best_H, best_split = 999, None\n",
    "      features = torch.randint(0, 4,(k,))\n",
    "\n",
    "      for feature_idx in features:\n",
    "          for threshold in torch.arange(0.15,7.9,0.1):\n",
    "              idx = X[:,feature_idx] > threshold\n",
    "              ####################################\n",
    "              # THIS LINE BELOW WILL REMOVE UNKNOWN OPCODE\n",
    "              ####################################\n",
    "              if idx.sum() == 0 or idx.sum() == idx.size(0):\n",
    "                  continue\n",
    "\n",
    "              m_left = idx.sum()\n",
    "              m_right = (~idx).sum()\n",
    "\n",
    "              H_left = self.entropy(y[idx])\n",
    "              H_right = self.entropy(y[~idx])\n",
    "              ## ANSWER\n",
    "              split_H = TODO\n",
    "              ## END ANSWER\n",
    "\n",
    "              ####################################\n",
    "              # THIS LINE BELOW WILL REMOVE UNKNOWN OPCODE\n",
    "              ####################################\n",
    "              if split_H < best_H or best_split == None:\n",
    "                  best_H, best_split = split_H, (feature_idx, threshold)\n",
    "      return best_split\n",
    "\n",
    "  def expand_node(self, node, X, y, max_depth=0, k=4):\n",
    "      # Completing the recursive call for building the decision tree\n",
    "      # node := Map(string: value) -- the tree represented as a Map, the key will take four\n",
    "      #   different string: 'label', 'split','left','right' (See implementation below)\n",
    "      #   'label': a label node with value as the mode of the labels\n",
    "      #   'split': the best split node with value as a tuple of feature id and threshold\n",
    "      #   'left','right': the left and right branch with value as the label node\n",
    "      # X := Tensor of size (m,d) -- Batch of m examples of demension d\n",
    "      # y := Tensor(int) of size (m) -- the given vectors of labels of the examples\n",
    "      # max_depth := int == the deepest level of the the decision tree\n",
    "      # k := int -- the number of classes, with default value as 4\n",
    "      # Return := tuple of (int, float) -- the feature id and threshold of the best split\n",
    "      #\n",
    "\n",
    "      H = self.entropy(y)\n",
    "      if H == 0 or max_depth == 0:\n",
    "          return\n",
    "\n",
    "      best_split = self.find_split(node, X, y, k=k)\n",
    "\n",
    "      ####################################\n",
    "      # THIS LINE BELOW WILL REMOVE UNKNOWN OPCODE\n",
    "      ####################################\n",
    "      if best_split == None:\n",
    "          return\n",
    "\n",
    "      idx = X[:,best_split[0]] > best_split[1]\n",
    "      X_left, y_left = X[idx], y[idx]\n",
    "      X_right, y_right = X[~idx], y[~idx]\n",
    "\n",
    "      del node['label']\n",
    "      node['split'] = best_split\n",
    "      node['left'] = { 'label': y_left.mode().values }\n",
    "      node['right'] = { 'label': y_right.mode().values }\n",
    "\n",
    "      # Fill in the following two lines to recursively build the rest of the\n",
    "      # decision tree\n",
    "      # self.expand_node(...)\n",
    "      # self.expand_node(...)\n",
    "      ## ANSWER\n",
    "\n",
    "      ## END ANSWER\n",
    "\n",
    "      return\n",
    "\n",
    "  def predict_one(self, node, x):\n",
    "      # Makes a prediction for a single example.\n",
    "      # node := Map(string: value) -- the tree represented as a Map, the key will take four\n",
    "      #   different string: 'label', 'split','left','right' (See implementation below)\n",
    "      #   'label': a label node with value as the mode of the labels\n",
    "      #   'split': the best split node with value as a tuple of feature id and threshold\n",
    "      #   'left','right': the left and right branch with value as the label node\n",
    "      # x := Tensor(float) of size(d,) -- the single example in a batch\n",
    "      # Fill in the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0c0WAqatpwW"
   },
   "outputs": [],
   "source": [
    "def fit_decision_tree(X,y, k=4):\n",
    "    # The function will fit data with decision tree with the expand_node method implemented above\n",
    "\n",
    "    root = { 'label': y.mode().values }\n",
    "    dt = DecisionTree()\n",
    "    dt.expand_node(root, X, y, max_depth=10, k=k)\n",
    "    return root\n",
    "\n",
    "def predict(node, X):\n",
    "    # return the predict result of the entire batch of examples using the predict_one function above.\n",
    "    dt = DecisionTree()\n",
    "    return torch.stack([dt.predict_one(node, x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-Lyl-r5CO2k"
   },
   "source": [
    "Test your code on the `iris` dataset. Your decision tree should fit to 100\\% training accuracy and generalize to 88\\% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1CZzJ_GLbUE"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "data=train_test_split(iris.data,iris.target,test_size=0.5,random_state=123)\n",
    "\n",
    "X,X_te,y,y_te = [torch.from_numpy(A) for A in data]\n",
    "X,X_te,y,y_te = X.float(), X_te.float(), y.long(), y_te.float()\n",
    "\n",
    "DT = fit_decision_tree(X,y,k=4)\n",
    "print('Train accuracy: ', (predict(DT, X) == y).float().mean().item())\n",
    "print('Test accuracy: ', (predict(DT, X_te) == y_te).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUDcIw1AOQWY"
   },
   "outputs": [],
   "source": [
    "\n",
    "grader.grade(test_case_id = 'entropy', answer = get_class_source(DecisionTree))\n",
    "grader.grade(test_case_id = 'find_split', answer = get_class_source(DecisionTree))\n",
    "grader.grade(test_case_id = 'expand_node', answer = get_class_source(DecisionTree))\n",
    "grader.grade(test_case_id = 'predict_one', answer = get_class_source(DecisionTree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slw1r1EInAl6"
   },
   "source": [
    "### Bagging Decision Trees for Random forests\n",
    "\n",
    "Note that our `find_split` implementation can use a random subset of the features when searching for the right split via the argument $k$. For the vanilla decision tree, we defaulted to $k=4$. Since there were 4 features, this meant that the decision tree could always considered all 4 features. By reducing the value of $k$ to $\\sqrt(k)=2$, we can introduce variance into the decision trees for the bagging algorithm.\n",
    "\n",
    "You'll now implement the bagging algorithm. Note that if you use the `clf` and `predict` functions given as keyword arguments, you can pass this section in the autograder without needing a correct implementation for decision trees from the previous section.\n",
    "\n",
    "1. Bootstrap (1pt): Implement `bootstrap` to draw a random bootstrap dataset from the given dataset.\n",
    "2. Fitting a random forest (1pt): Implement `random_forest_fit` to train a random forest that fits the data.\n",
    "3. Predicting with a random forest (1pt): Implement `predict_forest_fit` to make predictions given a random forest.\n",
    "\n",
    "Tip:\n",
    "+ If you're not sure whether your bootstrap is working or not, remember that on average, there will be $1-1/e\\approx 0.632$ unique samples in a bootstrapped dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tjqsx8cc7A3"
   },
   "outputs": [],
   "source": [
    "def bootstrap(X,y):\n",
    "    # Draw a random bootstrap dataset from the given dataset.\n",
    "    #\n",
    "    # X := Tensor(float) of size (m,d) -- Batch of m examples of demension d\n",
    "    # y := Tensor(int) of size (m) -- the given vectors of labels of the examples\n",
    "    #\n",
    "    # Return := Tuple of (Tensor(float) of size (m,d),Tensor(int) of size(m,)) -- the random bootstrap\n",
    "    #       dataset of X and its correcting lable Y\n",
    "    # Fill in the rest\n",
    "    pass\n",
    "\n",
    "def random_forest_fit(X, y, m, k, clf, bootstrap_func):\n",
    "    # Train a random forest that fits the data.\n",
    "    # X := Tensor(float) of size (n,d) -- Batch of n examples of demension d\n",
    "    # y := Tensor(int) of size (n) -- the given vectors of labels of the examples\n",
    "    # m := int -- number of trees in the random forest\n",
    "    # k := int -- number of classes of the features\n",
    "    # clf := function -- the decision tree model that the data will be trained on\n",
    "    # bootstrap := function -- the function to use for bootstrapping (pass in \"bootstrap\")\n",
    "    #\n",
    "    # Return := the random forest generated from the training datasets\n",
    "    # Fill in the rest\n",
    "    pass\n",
    "\n",
    "def random_forest_predict(X, clfs, predict):\n",
    "    # Implement `predict_forest_fit` to make predictions given a random forest.\n",
    "    # X := Tensor(float) of size (m,d) -- Batch of m examples of demension d\n",
    "    # clfs := list of functions -- the random forest\n",
    "    # predict := function that predicts (will default to your \"predict\" function)\n",
    "    # Return := Tensor(int) of size (m,) -- the predicted label from the random forest\n",
    "    # Fill in the rest\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_MAYCuQEKej"
   },
   "source": [
    "Test your code again on the `iris` dataset. Our random forest was able to improve the accuracy of the decision tree by about 10\\%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpFa5b_abG4S"
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "RF = random_forest_fit(X,y,50,2, clf=fit_decision_tree, bootstrap_func=bootstrap)\n",
    "\n",
    "print('Train accuracy: ', (random_forest_predict(X, RF, predict) == y).float().mean().item())\n",
    "print('Test accuracy: ', (random_forest_predict(X_te, RF, predict) == y_te).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlAcsMLjQJo6"
   },
   "outputs": [],
   "source": [
    "\n",
    "grader.grade(test_case_id = 'bootstrap', answer = getsource(bootstrap))\n",
    "grader.grade(test_case_id = 'random_forest_fit', answer = getsource(random_forest_fit))\n",
    "grader.grade(test_case_id = 'random_forest_predict', answer = getsource(random_forest_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqz8OEmbqacx"
   },
   "source": [
    "As a sanity check, the random forest can get around 95-97% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-snUdve2AMIc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "19NvT2AqjDMrVk6Gl65JUQGPdr8b7sW4j",
     "timestamp": 1739685596161
    }
   ]
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
