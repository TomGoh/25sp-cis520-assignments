{"cells":[{"cell_type":"markdown","metadata":{"id":"m1dkS0VksGp7"},"source":["# CIS 5200: Machine Learning\n","## Homework 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXZxyZDbr-Yx"},"outputs":[],"source":["import os\n","import sys\n","\n","# For autograder only, do not modify this cell.\n","# True for Google Colab, False for autograder\n","NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n","if NOTEBOOK:\n","    print(\"[INFO, OK] Google Colab.\")\n","else:\n","    print(\"[INFO, OK] Autograder.\")\n","    sys.exit()"]},{"cell_type":"markdown","metadata":{"id":"swECpqQGvLu9"},"source":["### Penngrader setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-peqcQNCvFSS"},"outputs":[],"source":["# %%capture\n","!pip install penngrader-client"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9VOzgVapPgrZ"},"outputs":[],"source":["%%writefile config.yaml\n","grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n","grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSSxUlaHvsrK"},"outputs":[],"source":["from penngrader.grader import PennGrader\n","\n","# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n","# TO ASSIGN POINTS TO YOU IN OUR BACKEND\n","STUDENT_ID = 00000000 # YOUR PENN-ID GOES HERE AS AN INTEGER #\n","SECRET = STUDENT_ID\n","\n","grader = PennGrader('config.yaml', 'cis5200_sp24_HW3', STUDENT_ID, SECRET)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRSiFAHsu0UQ"},"outputs":[],"source":["# packages for homework\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","from sklearn import datasets\n","import pandas as pd\n","\n","import matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"yK9J863mtwKz"},"source":["### 0. Gradients with PyTorch\n","\n","At this point, you've implemented a lot of gradients. However, these days, manual implementation of gradients is a thing of the past: PyTorch is a scientific computing library that comes with the ability to automatically compute gradients for you! This is called auto-differentiation. Here is an example of using auto-differentiation to compute the gradient of a quadratic function, $f(x) = ax^2$. The key parts are as follows:\n","\n","1. Variables that you want to differentiate with respect to should have the `requires_grad` flag set to `True`.\n","2. Calculate the objective that you'd like the compute the gradient of, using the variable from step (1).\n","3. Pass the objective and the variable you are differentiating to `torch.autograd.grad`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0c0WAqatpwW"},"outputs":[],"source":["# Step 1: Set requires_grad to true for x\n","x = torch.Tensor([3.0])\n","a = torch.Tensor([1.5])\n","x.requires_grad = True\n","\n","# Step 2: Compute the objective\n","y = a*(x**2)\n","\n","# Step 3: Use autograd\n","grad = torch.autograd.grad([y],[x])[0]\n","\n","print(\"PyTorch gradient:\", grad)\n","print(\"Analytic gradient:\", 2*a*x)"]},{"cell_type":"markdown","metadata":{"id":"_Kn7XtkCwL-L"},"source":["You'll notice that the gradient computed with PyTorch matches exactly the analytic gradient $\\nabla f(x) = 2ax$, but without having to implement or derive the analytic gradient! This works for gradients with respect to any sized variables. For example, if $x$ is now a vector, and the objective is $f(x) = a\\|x\\|_2^2$ then we can calculate the gradient in the same way:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qB9LrKzMw6Q8"},"outputs":[],"source":["# Step 1: Set requires_grad to true for x\n","x = torch.Tensor([3.0, 2.0])\n","a = torch.Tensor([1.5])\n","x.requires_grad = True\n","\n","# Step 2: Compute the objective\n","y = a*(x.norm(p=2)**2)\n","\n","# Step 3: Use autograd\n","grad = torch.autograd.grad([y],[x])[0]\n","\n","print(\"PyTorch gradient:\", grad)\n","print(\"Analytic gradient:\", 2*a*x)"]},{"cell_type":"markdown","metadata":{"id":"84woF5SHxVAU"},"source":["From now on, we highly recommend that you use auto-differentiation to calculate gradients. As long as all of your operations are differentiable, the final objective will be differentiable.\n"]},{"cell_type":"markdown","metadata":{"id":"W7LWmjB_64uO"},"source":["# 1. SVM and Gradient Descent\n","\n","In this first problem, you'll implement (soft margin) support vector machines with gradient descent, using gradients from PyTorch's autodifferentiation library.\n","+ (2pts) Calculate the objective of the Soft SVM\n","+ (2pts) Calculate the gradient of the Soft SVM objective\n","+ (2pts) Implement a basic gradient descent optimizer. Your solution needs to converge to an accurate enough answer.\n","+ (1pts) Make predictions with the Soft SVM\n","\n","Tips:\n","- This assignment is more freeform than previous ones. You're allowed to initialize the parameters of the SVM model however you want, as long as your implemented functions return the right values.\n","- We recommend using PyTorch's `torch.autograd.grad` to get the gradient instead of deriving the SVM gradient.\n","- You'll need to play with the values of step size and number of iterations to\n","converge to a good value.\n","- To debug your optimization, print the objective over iterations. Remember that the theory says as long as the learning rate is small enough, for strongly convex problems, we are guaranteed to converge at a certain rate. What does this imply about your solution if it is not converging?\n","- As a sanity check, you can get around 97.5% prediction accuracy and converge to an objective below 0.16.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppRNBbiY84N5"},"outputs":[],"source":["class SoftSVM():\n","    def __init__(self, ndims):\n","        # Here, we initialize the parameters of your soft-SVM model for binary\n","        # classification. You can change the initialization but don't change\n","        # the weight and bias variables as the autograder will assume that\n","        # these exist.\n","        # ndims := integer -- number of dimensions\n","        # no return type\n","\n","        self.weight = torch.randn(ndims)\n","        self.bias = torch.randn(1)\n","        self.weight.requires_grad = True\n","        self.bias.requires_grad = True\n","\n","    def objective(self, X, y, l2_reg):\n","        # Calculate the objective of your soft-SVM model\n","        # X := Tensor of size (m,d) -- the input features of m examples with d dimensions\n","        # y := Tensor of size (m) -- the labels for each example in X\n","        # l2_reg := float -- L2 regularization penalty\n","        # Returns a scalar tensor (zero dimensional tensor) -- the loss for the model\n","        # Fill in the rest\n","        pass\n","        return torch.zeros(1)\n","\n","\n","    def gradient(self, X, y, l2_reg):\n","        # Calculate the gradient of your soft-SVM model\n","        # X := Tensor of size (m,d) -- the input features of m examples with d dimensions\n","        # y := Tensor of size (m) -- the labels for each example in X\n","        # l2_reg := float -- L2 regularization penalty\n","        # Return Tuple (Tensor, Tensor) -- the tensors corresponds to the\n","        # gradients of the weight and bias parameters respectively\n","        # Fill in the rest\n","        pass\n","        return torch.zeros_like(self.weight), torch.zeros_like(self.bias)\n","\n","    def optimize(self, X, y, l2_reg):\n","        # Calculate the gradient of your soft-SVM model\n","        # X := Tensor of size (m,d) -- the input features of m examples with d dimensions\n","        # y := Tensor of size (m) -- the labels for each example in X\n","        # l2_reg := float -- L2 regularization penalty\n","\n","        # no return type\n","\n","        # Fill in the rest\n","        pass\n","\n","    def predict(self, X):\n","        # Given an X, make a prediction with the SVM\n","        # X := Tensor of size (m,d) -- features of m examples with d dimensions\n","        # Return a tensor of size (m) -- the prediction labels on the dataset X\n","\n","        # Fill in the rest\n","        pass\n","        return torch.zeros(X.size(0))"]},{"cell_type":"markdown","metadata":{"id":"xVxoFHtpEXtx"},"source":["Test the Soft SVM on the breast cancer dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6EujmjUe_dQx"},"outputs":[],"source":["#Load dataset\n","cancer = datasets.load_breast_cancer()\n","X,y = torch.from_numpy(cancer['data']), torch.from_numpy(cancer['target'])\n","mu,sigma = X.mean(0,keepdim=True), X.std(0,keepdim=True)\n","X,y = ((X-mu)/sigma).float(),(y - 0.5).sign() # prepare data\n","l2_reg = 0.1\n","print(X.size(), y.size())\n","\n","# Optimize the soft-SVM with gradient descent\n","clf = SoftSVM(X.size(1))\n","clf.optimize(X,y,l2_reg)\n","print(\"\\nSoft SVM objective: \")\n","print(clf.objective(X,y,l2_reg).item())\n","print(\"\\nSoft SVM accuracy: \")\n","(clf.predict(X) == y).float().mean().item()"]},{"cell_type":"code","source":["grader.grade(test_case_id = 'SVM_objective', answer = SoftSVM)\n","grader.grade(test_case_id = 'SVM_gradient', answer = SoftSVM)\n","grader.grade(test_case_id = 'SVM_optimize', answer = SoftSVM)\n","grader.grade(test_case_id = 'SVM_predict', answer = SoftSVM)"],"metadata":{"id":"YoT4dRVBDjsy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Extending Reverse Mode Auto-differentiation (12 points)\n","\n","In lecture we learned about how auto-differentiation could be used to automatically calculate gradients through a computational graph. Crucially, all we needed to do was know how to propagate variations of the chain rule at each individual node in the graph.\n","\n","The PyTorch framework makes it very simple to extend auto-differentation to new formula or expressions. Specifically, PyTorch keeps track of the computational graph with the `forward` function, saving relevant computations, and then computes the chain rule with the `backward` function (in reverse mode).\n","\n","For example, consider the Legendre polynomial of degree 3, $P_3(x) = \\frac{1}{2}(5x^3 - 3x)$. How would we implement our own custom module to do this, if it didn't already exist in PyTorch? We can do this like in the following ([example taken from the PyTorch documetnation](https://pytorch.org/docs/stable/notes/extending.html)):  "],"metadata":{"id":"JmyVGWOfPkS_"}},{"cell_type":"code","source":["class LegendrePolynomial3(torch.autograd.Function):\n","    \"\"\"\n","    We can implement our own custom autograd Functions by subclassing\n","    torch.autograd.Function and implementing the forward and backward passes\n","    which operate on Tensors.\n","    \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, input):\n","        \"\"\"\n","        In the forward pass we receive a Tensor containing the input and return\n","        a Tensor containing the output. ctx is a context object that can be used\n","        to stash information for backward computation. You can cache arbitrary\n","        objects for use in the backward pass using the ctx.save_for_backward method.\n","        \"\"\"\n","        ctx.save_for_backward(input)\n","        return 0.5 * (5 * input ** 3 - 3 * input)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        \"\"\"\n","        In the backward pass we receive a Tensor containing the gradient of the loss\n","        with respect to the output, and we need to compute the gradient of the loss\n","        with respect to the input.\n","        \"\"\"\n","        input, = ctx.saved_tensors\n","        return grad_output * 1.5 * (5 * input ** 2 - 1)"],"metadata":{"id":"wEjxtGuvPk38"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And that's it! We can now use the `LegendrePolynomial3` function in combination with any other PyTorch functions and automatically calculate gradients with auto-differentiation. Note that, since PyTorch uses *reverse-mode*, in the `backward` function we are:  \n","1. Given the gradient of the loss with respect to the output of the function, and\n","2. Need to recompute the gradient of the loss with respect to the inputs of the function.\n","\n","This is an extremely general and powerful framework. For example, researchers have implemented forward functions and their gradients for modules as complex as a call to an external simulator such as a physics engine, which would then let you differentiate through the simulator!\n","\n","In this exercise, you'll implement a new PyTorch function for a simpler function, the integral:\n","$$f(x) = \\int_{-\\infty}^x g(t)dt$$\n","PyTorch doesn't have a function for doing numerical integration, so we'll need to implement the backwards function ourselves if we want to use auto-differentiate with integrals. In particular, we'll do this for the following piece-wise constant function:\n","\n","$$g(x; \\beta, \\eta) = \\sum_{r=1}^R \\beta_r \\mathbb 1[x\\in (\\eta_{r-1},\\eta_r)]$$\n","\n","for parameters $\\beta\\in \\mathbb R^R$ and $\\eta\\in \\mathbb R^{R+1}$.\n","\n","1. Forward (2pts): Implement the `forward` function which calculates $f(x)$ for a batch of example $x$.\n","2. Backward (6pts): Implement the `backward` function which calculates the reverse-mode auto-differentiation rule for $f$ with respect to $x,\\beta, \\eta$, 2 points each.\n","\n","Hints:\n","+ Recall that $g(x)$ is a piece-wise constant function, and the integral of a function is the area between the function and 0. Thus, you can reformulate the integral of each constant part of $g$ as simply the (signed) area of the rectangle, and so the integral of $g(x)$ is the sum of the areas of all the piece-wise rectangles up to $x$\n","+ Recall that for reverse-mode autodifferentiation, we are given the derivative of the loss with respect to the output, or $\\frac{\\partial \\ell(x))}{\\partial f(x; \\beta, \\eta)}$, and our goal is to then compute $\\frac{\\partial \\ell(x))}{\\partial x}, \\frac{\\partial \\ell(x))}{\\partial \\beta}, \\frac{\\partial \\ell(x))}{\\partial \\eta}$.\n","+ Remember the chain rule: $$\\frac{\\partial \\ell(x))}{\\partial x} = \\sum_i \\frac{\\partial \\ell(x))}{\\partial f(x; \\beta, \\eta)_i} \\cdot \\frac{\\partial f(x; \\beta, \\eta))_i}{\\partial x}$$\n","+ For simplicity, you can assume that all $x$'s and $\\eta$'s are distinct so you don't have to worry about border cases for the intervals."],"metadata":{"id":"hsFCXkjyPn_Z"}},{"cell_type":"code","source":["class IntegralPiecewiseConstant(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, X, beta, eta):\n","        # ctx := A PyTorch object for saving data for use in the backward\n","        #   function\n","        # X := Tensor(float) of size (m) -- a minibatch of examples\n","        # beta := Tensor(float) of size (k) -- the magnitudes of each part of\n","        #   the piece-wise constant function\n","        # eta := Tensor(float) of size (k+1) -- the start/end points of each\n","        #   part of the piece-wise constant function\n","        # Return the integral of the piece-wise constant function applied to\n","        #   each entry of  X\n","\n","        # Here we save the inputs to the forward function to use in backward\n","        ctx.save_for_backward(X, beta, eta)\n","\n","        # Fill in the rest\n","        pass\n","        return torch.zeros(X.size(0))\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        # ctx := A PyTorch object for containing saved data from the forward\n","        #   function\n","        # grad_output := Tensor(float) of size (m) -- a minibatch of gradients\n","        #   of the loss with respect to the output of this function. Since we\n","        #   are working with a scalar function, each element is the gradient\n","        #   with respect to an output of the minibatch. In other words, this is\n","        #   dL/df(x) where this module outputs f(x).\n","        # Return a tuple containing the gradient of the loss with respect to\n","        #   the inputs X, beta, and eta: (dL/dX, dL/dbeta, dL/deta).\n","\n","        # Here we retrieve the tensors from the forward pass\n","        X, beta, eta = ctx.saved_tensors\n","        print(grad_output)\n","\n","        # Fill in the rest\n","        pass\n","        return torch.zeros_like(X), torch.zeros_like(beta), torch.zeros_like(eta)"],"metadata":{"id":"f4tNbruOPnwR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As an example, calculating the gradients for the following example will result in the following output:\n","```\n","tensor([ 1., -1.,  3.])\n","tensor([0.2500, 0.4000, 0.7000, 0.1000])\n","tensor([-3., -2.,  6., -4.,  0.])\n","```"],"metadata":{"id":"KB7NXfAMPtcJ"}},{"cell_type":"code","source":["X = torch.Tensor([0.05,0.5,0.9])\n","betas = torch.Tensor([1,2,-1,3])\n","etas = torch.Tensor([0,0.1,0.3,0.8,1])\n","\n","X.requires_grad = True\n","betas.requires_grad = True\n","etas.requires_grad = True\n","IntegralPiecewiseConstant.apply(X, betas, etas).sum().backward()\n","\n","print(X.grad)\n","print(betas.grad)\n","print(etas.grad)"],"metadata":{"id":"uqlAqg6LPu6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grader.grade(test_case_id = 'autodiff_forward', answer = IntegralPiecewiseConstant.forward)\n","grader.grade(test_case_id = 'autodiff_backward', answer = IntegralPiecewiseConstant.backward)"],"metadata":{"id":"IFVintChPz0b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Neural Networks and Gradient Descent\n","\n","In the previous example, we directly calculated the gradient of a function with respect to various inputs using PyTorch's `autograd` library. As we did in that problem, one can use this autograd library to directly implement gradient descent by iterating over all parameters and applying the gradient update. However, as the number of parameters grow, directly implementing these updates can become quite onerous. To handle neural networks with lots of parameters, the PyTorch library includes optimizers that make training with gradient descent very easy with the following 5 steps:\n","1. Create an optimizer object and give it all the parameters you'd like to optimize\n","2. Calculate a loss that you'd like to minimize\n","3. Clear old gradients\n","4. Calculate new gradients\n","5. Update the parameters with one gradient step\n","\n","The end result is a generic boilerplate recipe that will optimize *any* objective with gradient descent. Here is an example running gradient descent on a linear model, using the stochastic gradient descent (SGD) optimizer and a basic dataloader."],"metadata":{"id":"82AEAjkDP8Vm"}},{"cell_type":"code","source":["# Setup a simple problem\n","m,d = 128,5\n","X = torch.randn(m,d)\n","w_opt, b_opt = torch.randn(d), torch.randn(1)\n","y = X.matmul(w_opt) + b_opt\n","\n","# setup the dataloader\n","simple_dataset = torch.utils.data.TensorDataset(X,y)\n","loader = torch.utils.data.DataLoader(simple_dataset,batch_size=16)\n","\n","# Create the model\n","lin = nn.Linear(d,1)\n","\n","# setup the optimizer (Step 1)\n","opt = torch.optim.SGD(lin.parameters(), lr=0.001)\n","\n","# iterate over epochs\n","for i in range(100):\n","    # iterate over minibatches\n","    for X0,y0 in loader:\n","        yhat = lin(X0).squeeze(1) # make predictions\n","        loss = F.mse_loss(yhat,y0) # calculate loss (Step 2)\n","\n","        opt.zero_grad() # clear gradients from previous iteration (Step 3)\n","        loss.backward() # calculate new gradients (Step 4)\n","        opt.step() # update parameters (Step 5)\n","\n","    # logging\n","    with torch.no_grad():\n","        if i % 10 == 0:\n","            print(loss.item())"],"metadata":{"id":"0pnA6QH8AKBo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","In the second part of this assignment, we'll implement a basic neural network for a tree cover classification problem using the PyTorch library. Here, the problem is to use 12 features (which have been expanded to 54 columns of data to expand categorical variables into binary features) to predict one of 7 tree cover types. A full description of the dataset can be found [here](http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info). The following cell will download the data and convert it into PyTorch tensors for you, with a training split of 2000 examples per class and a validation split of 500 examples per class."],"metadata":{"id":"FL9ycJUKP_t3"}},{"cell_type":"code","source":["!pip install ucimlrepo\n","\n","from ucimlrepo import fetch_ucirepo\n","\n","# fetch dataset\n","covertype = fetch_ucirepo(id=31)\n","\n","# data (as pandas dataframes)\n","X = covertype.data.features\n","y = covertype.data.targets\n","\n","# metadata\n","# print(covertype.metadata)\n","\n","# variable information\n","# print(covertype.variables)"],"metadata":{"id":"hVVKICOHTEfi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.concat([covertype.data.features, covertype.data.targets], axis = 1)\n","df = pd.concat([df.iloc[df.values[:,-1]==i].sample(2500) for i in range(1,8)])\n","X,y = df.values[:,:-1], df.values[:,-1]-1 # re-index labels to 0-6\n","X,y = [torch.from_numpy(a) for a in (X,y)] # convert to PyTorch\n","\n","dataset = torch.utils.data.TensorDataset(X,y)\n","train_set,val_set = torch.utils.data.random_split(dataset,[2000*7,500*7]) # generate splits"],"metadata":{"id":"jB6JPHP4T-hP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Your goal is to achieve at least 70% accuracy on this forest cover task. We suggest a very simple neural network.\n","\n","1. (5pts) Implement a model for predicting forest cover. You will get 1 point for every first 14% accuracy, up to 70% for a total of 5 points. This is achievable with a small neural network with one hidden layer and ReLU activations. It is possible to achieve over 80% accuracy with less than 15 seconds (many solutions exist that take less than a minute).\n","\n","It is possible to get much higher than 75%, in fact it is possible to exceed 80% accuracy. **You do not need a GPU**.\n","\n","Hints:\n","+ A simple neural network directly on the data can get around 50% accuracy. You can start with a sequential model, linear layers, and ReLU activations (PyTorch has a wide range of modules [here](https://pytorch.org/docs/stable/nn.html)).\n","+ To iterate over minibatches, PyTorch has a useful `DataLoader` object with documentation [here](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader).\n","+ Data normalization can be a very important factor, not just in deep learning. Check your dataset statistics and normalize to improve your accuracy further.\n","+ You may need to explore different types of optimizers and learning rates.  \n","+ Remember to check your performance on the validation set, and not the training set. You will need to perform well on the held-out test data.\n","+ To help with your testing, you can add additional parameters via keyword arguments to tweak your pipeline.\n","+ There is no single right answer here---the only goal is to get to 70%!\n","+ Solutions that exploit the grader to extract test set labels will receive no credit."],"metadata":{"id":"2WHjW_JdQGNf"}},{"cell_type":"code","source":["class ForestCover():\n","    def __init__(self, train_set):\n","        # train_set := a PyTorch dataset of training examples from the tree\n","        #   cover prediction problem.\n","\n","        # Initialize your model here!\n","        pass\n","\n","    def train(self, train_set):\n","        # train_set := a PyTorch dataset of training examples from the tree\n","        #   cover prediction problem.\n","\n","        # Train your model here!\n","        pass\n","\n","    def predict(self, X):\n","        # X := a Tensor(float) of size (m,d) of examples for the model to\n","        #   to predict.\n","\n","        # Make predictions on a new input here!\n","        return torch.randn(X.size(0))\n","\n","solution = ForestCover(train_set)\n","solution.train(train_set)\n","\n","acc = []\n","for X,y in torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False):\n","    y_pred = solution.predict(X)\n","    acc.append(y_pred == y)\n","\n","acc = torch.cat(acc).float().mean()\n","print(f\"Validation accuracy: {acc:.2f}\")"],"metadata":{"id":"UtzdW42oQHp6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Your code will be scored according to accuracy on a held-out test set.\n","If you don't use your validation set during training, your performance on the validation set will be approximately your performance on the test set.\n","**Warning: solutions that exploit the grader to extract test set labels will receive a manual adjustment for zero credit.**"],"metadata":{"id":"LO5DHi9OMkTB"}},{"cell_type":"code","source":["!wget https://machine-learning-upenn.github.io/assets/hw3/X_test.pth -O \"X_test.pth\"\n","X_test = torch.load(\"X_test.pth\")"],"metadata":{"id":"d_XlEvpP4WC3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_soln = solution.predict(X_test)\n","grader.grade(test_case_id = 'forestcover', answer = y_soln)"],"metadata":{"id":"0gdUbvJ3HH7y"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.1"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}